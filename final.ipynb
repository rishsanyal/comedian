{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding everything from Step-1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "\n",
    "## Read in joke id and joke text\n",
    "jester_joke_data = pd.read_csv('datasets/jester_items.csv')\n",
    "\n",
    "## Read in joke ratings\n",
    "jester_joke_ratings = pd.read_csv('datasets/jester_ratings.csv')\n",
    "\n",
    "## Read reddit jokes and unify the format for the joke data\n",
    "reddit_jokes = pd.read_json('datasets/reddit_jokes.json')\n",
    "reddit_jokes['jokeId'] = reddit_jokes.id\n",
    "\n",
    "## TODO: Add this dataframe to the jester_joke_data dataframe and update the index\n",
    "reddit_jokes['jokeText'] = reddit_jokes['title'] + '\\n' + reddit_jokes['body']\n",
    "reddit_joke_data = reddit_jokes[['jokeId', 'jokeText']]\n",
    "\n",
    "## Read stupidstuff.json and unify the format for the joke data\n",
    "stupidstuff_jokes = pd.read_json('datasets/stupidstuff_shuffled.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = [0, 1, 2]\n",
    "\n",
    "user_ratings_files = [ 'datasets/ratings_USF100' + str(user_id) + '.csv' for user_id in user_ids ]\n",
    "\n",
    "user_ratings = pd.concat([pd.read_csv(user_ratings_file) for user_ratings_file in user_ratings_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new columns for all the jokes accoring to the user ratings\n",
    "\n",
    "unique_user_ids = user_ratings['user_id'].unique()\n",
    "\n",
    "for index, row in user_ratings.iterrows():\n",
    "\n",
    "    user_num = row['user_id']\n",
    "    joke_id = row['joke_id']\n",
    "\n",
    "    joke_ratings = user_ratings[(user_ratings['joke_id'] == joke_id)]\n",
    "\n",
    "    for user_id in unique_user_ids:\n",
    "        ratings = joke_ratings[joke_ratings['user_id'] == user_id]['rating'].values\n",
    "        if len(ratings) > 0:\n",
    "            rating = int(ratings[0])\n",
    "        else:\n",
    "            rating = np.nan\n",
    "        user_ratings.loc[index, (user_id + \"_rating\")] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings.drop_duplicates(subset='joke_id', inplace=True)\n",
    "\n",
    "user_ratings.drop(['user_id', 'rating'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_id                 0\n",
      "USF1000_rating        221\n",
      "USF1001_rating         77\n",
      "USF1002_rating          0\n",
      "source                  0\n",
      "datasource_joke_id      0\n",
      "dtype: int64\n",
      "joke_id               0\n",
      "USF1000_rating        0\n",
      "USF1001_rating        0\n",
      "USF1002_rating        0\n",
      "source                0\n",
      "datasource_joke_id    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(user_ratings)\n",
    "\n",
    "# user_ratings.joke_id.unique()\n",
    "\n",
    "def add_source_column(row):\n",
    "    joke_id = row['joke_id']\n",
    "\n",
    "    if joke_id.startswith('RE'):\n",
    "        temp_joke_id = joke_id[2:]\n",
    "        row['source'] = \"reddit\"\n",
    "        row['datasource_joke_id'] = temp_joke_id\n",
    "        # user_ratings.loc[index, 'source'] = \"reddit\"\n",
    "        # user_ratings.loc[index, 'datasource_joke_id'] = temp_joke_id\n",
    "\n",
    "    elif joke_id.startswith('JE'):\n",
    "        temp_joke_id = joke_id[2:]\n",
    "        row['source'] = \"jester\"\n",
    "        row['datasource_joke_id'] = temp_joke_id\n",
    "\n",
    "    elif joke_id.startswith('SS'):\n",
    "        temp_joke_id = joke_id[2:]\n",
    "        row['source'] = \"stupidstuff\"\n",
    "        row['datasource_joke_id'] = temp_joke_id\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "user_ratings = user_ratings.apply(add_source_column, axis=1)\n",
    "\n",
    "print(user_ratings.isna().sum())\n",
    "\n",
    "na_user_ratings = user_ratings[user_ratings.isna().any(axis=1)]\n",
    "\n",
    "user_ratings.dropna(inplace=True)\n",
    "\n",
    "print(user_ratings.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the joke text with the user ratings\n",
    "\n",
    "def merge_joke_text(row):\n",
    "    joke_id = row['datasource_joke_id']\n",
    "    joke_source = row['source']\n",
    "\n",
    "    if joke_source == 'reddit':\n",
    "        joke_text = reddit_joke_data[reddit_joke_data['jokeId'] == joke_id]['jokeText'].values\n",
    "        if joke_text:\n",
    "            row['jokeText'] = joke_text[0]\n",
    "        else:\n",
    "            print (\"Joke not found for reddit with id: \" + joke_id)\n",
    "        \n",
    "    elif joke_source == 'jester':\n",
    "        joke_text = jester_joke_data[jester_joke_data['jokeId'] == int(joke_id)]['jokeText'].values\n",
    "        if joke_text:\n",
    "            row['jokeText'] = joke_text[0]\n",
    "        else:\n",
    "            print (\"Joke not found for jester with id->\" + joke_id)\n",
    "        \n",
    "    elif joke_source == 'stupidstuff':\n",
    "        joke_id = int(joke_id)\n",
    "        joke_text = stupidstuff_jokes[stupidstuff_jokes['jokeId'] == joke_id]['jokeText'].values\n",
    "        \n",
    "        if joke_text:\n",
    "            row['jokeText'] = joke_text[0]\n",
    "        else:\n",
    "            print (\"Joke not found for stupidstuff with id: \" + joke_id)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "temp_df = user_ratings.apply(merge_joke_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(temp_df['source'])\n",
    "\n",
    "temp_df = temp_df.join(one_hot)\n",
    "\n",
    "joke_info_dataset = temp_df.drop('source',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>country</th>\n",
       "      <th>location</th>\n",
       "      <th>avg_user_rating</th>\n",
       "      <th>city</th>\n",
       "      <th>buddy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USF1000</td>\n",
       "      <td>Tanya</td>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>South East Asian</td>\n",
       "      <td>India</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>5.968586</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>USF1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USF1001</td>\n",
       "      <td>rishab</td>\n",
       "      <td>26</td>\n",
       "      <td>Male</td>\n",
       "      <td>South East Asian</td>\n",
       "      <td>India</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>6.727749</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>USF1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USF1002</td>\n",
       "      <td>Parisa</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>Middle Eastern</td>\n",
       "      <td>Iran</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0.806283</td>\n",
       "      <td>Ahvaz</td>\n",
       "      <td>USF1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    name  age  gender         ethnicity country       location  \\\n",
       "0  USF1000   Tanya   24  Female  South East Asian   India  San Francisco   \n",
       "1  USF1001  rishab   26    Male  South East Asian   India  San Francisco   \n",
       "2  USF1002  Parisa   26  Female    Middle Eastern    Iran  San Francisco   \n",
       "\n",
       "   avg_user_rating    city    buddy  \n",
       "0         5.968586   Delhi  USF1001  \n",
       "1         6.727749  Mumbai  USF1000  \n",
       "2         0.806283   Ahvaz  USF1001  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_info = pd.read_csv('datasets/users.csv')\n",
    "\n",
    "users_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on Step 2 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_id</th>\n",
       "      <th>USF1000_rating</th>\n",
       "      <th>USF1001_rating</th>\n",
       "      <th>USF1002_rating</th>\n",
       "      <th>source</th>\n",
       "      <th>datasource_joke_id</th>\n",
       "      <th>jokeText</th>\n",
       "      <th>jester</th>\n",
       "      <th>reddit</th>\n",
       "      <th>stupidstuff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE5tz52q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>I hate how you cant even say black paint anymo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE5tz4dd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz4dd</td>\n",
       "      <td>What's the difference between a Jew in Nazi Ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE5tz319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz319</td>\n",
       "      <td>I recently went to America....\\n...and being t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RE5tz2wj</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz2wj</td>\n",
       "      <td>Brian raises his hand and says, “He’s in Heave...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RE5tz1pc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz1pc</td>\n",
       "      <td>You hear about the University book store worke...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>SS2771</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>2771</td>\n",
       "      <td>Q: How many Field Service Engineers does it ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>SS1365</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>1365</td>\n",
       "      <td>Yo mamma so dumb she sold her car for gasoline...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>SS570</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>570</td>\n",
       "      <td>A woman gets home, schreeches her car into the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>SS431</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>431</td>\n",
       "      <td>The psychology instructor had just finished a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SS1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>1460</td>\n",
       "      <td>Your mama is so fat, she fell in love and brok...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      joke_id  USF1000_rating  USF1001_rating  USF1002_rating       source  \\\n",
       "0    RE5tz52q             1.0             8.0             0.0       reddit   \n",
       "1    RE5tz4dd             1.0             8.0             0.0       reddit   \n",
       "2    RE5tz319             0.0             9.0             8.0       reddit   \n",
       "3    RE5tz2wj             3.0             9.0             0.0       reddit   \n",
       "4    RE5tz1pc             5.0             3.0             8.0       reddit   \n",
       "..        ...             ...             ...             ...          ...   \n",
       "136    SS2771             7.0            10.0             0.0  stupidstuff   \n",
       "137    SS1365             8.0            10.0             0.0  stupidstuff   \n",
       "138     SS570             8.0             3.0             0.0  stupidstuff   \n",
       "139     SS431             7.0             7.0             0.0  stupidstuff   \n",
       "140    SS1460             0.0             6.0             0.0  stupidstuff   \n",
       "\n",
       "    datasource_joke_id                                           jokeText  \\\n",
       "0               5tz52q  I hate how you cant even say black paint anymo...   \n",
       "1               5tz4dd  What's the difference between a Jew in Nazi Ge...   \n",
       "2               5tz319  I recently went to America....\\n...and being t...   \n",
       "3               5tz2wj  Brian raises his hand and says, “He’s in Heave...   \n",
       "4               5tz1pc  You hear about the University book store worke...   \n",
       "..                 ...                                                ...   \n",
       "136               2771  Q: How many Field Service Engineers does it ta...   \n",
       "137               1365  Yo mamma so dumb she sold her car for gasoline...   \n",
       "138                570  A woman gets home, schreeches her car into the...   \n",
       "139                431  The psychology instructor had just finished a ...   \n",
       "140               1460  Your mama is so fat, she fell in love and brok...   \n",
       "\n",
       "     jester  reddit  stupidstuff  \n",
       "0         0       1            0  \n",
       "1         0       1            0  \n",
       "2         0       1            0  \n",
       "3         0       1            0  \n",
       "4         0       1            0  \n",
       "..      ...     ...          ...  \n",
       "136       0       0            1  \n",
       "137       0       0            1  \n",
       "138       0       0            1  \n",
       "139       0       0            1  \n",
       "140       0       0            1  \n",
       "\n",
       "[165 rows x 10 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = temp_df\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")\n",
    "pipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and fill it with the predicted probability for each row\n",
    "def assign_labels(model, df, label_names):\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['jokeText']\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        softmax = torch.softmax(logits, dim=1)\n",
    "        predicted_probabilities = softmax.detach().numpy()[0]\n",
    "        for i, label in enumerate(label_names):\n",
    "            df.at[index, label] = predicted_probabilities[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in ['unitary/toxic-bert', 'michellejieli/emotion_text_classifier']:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    label_names = list(model.config.id2label.values())\n",
    "    df = assign_labels(model, df, label_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non-misogynist', 'misogynist']\n",
      "['not sexist', 'sexist']\n"
     ]
    }
   ],
   "source": [
    "# MilaNLProc/bert-base-uncased-ear-misogyny\n",
    "for model_path in ['MilaNLProc/bert-base-uncased-ear-misogyny', 'NLP-LTU/distilbert-sexism-detector']:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    label_names = list(model.config.id2label.values())\n",
    "    print(label_names)\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['jokeText']\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        softmax = torch.softmax(logits, dim=1)\n",
    "        predicted_probabilities = softmax.detach().numpy()[0]\n",
    "        for i, label in enumerate(label_names):\n",
    "            if label in ['misogynist', 'sexist']:\n",
    "                df.at[index, label] = predicted_probabilities[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "This is where we calculate the metrics for every demographic\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_id</th>\n",
       "      <th>USF1000_rating</th>\n",
       "      <th>USF1001_rating</th>\n",
       "      <th>USF1002_rating</th>\n",
       "      <th>source</th>\n",
       "      <th>datasource_joke_id</th>\n",
       "      <th>jokeText</th>\n",
       "      <th>jester</th>\n",
       "      <th>reddit</th>\n",
       "      <th>stupidstuff</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_Middle Eastern</th>\n",
       "      <th>ethnicity_South East Asian</th>\n",
       "      <th>country_India</th>\n",
       "      <th>country_Iran</th>\n",
       "      <th>location_San Francisco</th>\n",
       "      <th>city_Ahvaz</th>\n",
       "      <th>city_Delhi</th>\n",
       "      <th>city_Mumbai</th>\n",
       "      <th>buddy_USF1000</th>\n",
       "      <th>buddy_USF1001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE5tz52q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>I hate how you cant even say black paint anymo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE5tz4dd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz4dd</td>\n",
       "      <td>What's the difference between a Jew in Nazi Ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE5tz319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz319</td>\n",
       "      <td>I recently went to America....\\n...and being t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RE5tz2wj</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz2wj</td>\n",
       "      <td>Brian raises his hand and says, “He’s in Heave...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RE5tz1pc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>5tz1pc</td>\n",
       "      <td>You hear about the University book store worke...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>SS2771</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>2771</td>\n",
       "      <td>Q: How many Field Service Engineers does it ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>SS1365</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>1365</td>\n",
       "      <td>Yo mamma so dumb she sold her car for gasoline...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>SS570</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>570</td>\n",
       "      <td>A woman gets home, schreeches her car into the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>SS431</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>431</td>\n",
       "      <td>The psychology instructor had just finished a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SS1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupidstuff</td>\n",
       "      <td>1460</td>\n",
       "      <td>Your mama is so fat, she fell in love and brok...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      joke_id  USF1000_rating  USF1001_rating  USF1002_rating       source  \\\n",
       "0    RE5tz52q             1.0             8.0             0.0       reddit   \n",
       "1    RE5tz4dd             1.0             8.0             0.0       reddit   \n",
       "2    RE5tz319             0.0             9.0             8.0       reddit   \n",
       "3    RE5tz2wj             3.0             9.0             0.0       reddit   \n",
       "4    RE5tz1pc             5.0             3.0             8.0       reddit   \n",
       "..        ...             ...             ...             ...          ...   \n",
       "136    SS2771             7.0            10.0             0.0  stupidstuff   \n",
       "137    SS1365             8.0            10.0             0.0  stupidstuff   \n",
       "138     SS570             8.0             3.0             0.0  stupidstuff   \n",
       "139     SS431             7.0             7.0             0.0  stupidstuff   \n",
       "140    SS1460             0.0             6.0             0.0  stupidstuff   \n",
       "\n",
       "    datasource_joke_id                                           jokeText  \\\n",
       "0               5tz52q  I hate how you cant even say black paint anymo...   \n",
       "1               5tz4dd  What's the difference between a Jew in Nazi Ge...   \n",
       "2               5tz319  I recently went to America....\\n...and being t...   \n",
       "3               5tz2wj  Brian raises his hand and says, “He’s in Heave...   \n",
       "4               5tz1pc  You hear about the University book store worke...   \n",
       "..                 ...                                                ...   \n",
       "136               2771  Q: How many Field Service Engineers does it ta...   \n",
       "137               1365  Yo mamma so dumb she sold her car for gasoline...   \n",
       "138                570  A woman gets home, schreeches her car into the...   \n",
       "139                431  The psychology instructor had just finished a ...   \n",
       "140               1460  Your mama is so fat, she fell in love and brok...   \n",
       "\n",
       "     jester  reddit  stupidstuff  ...  ethnicity_Middle Eastern  \\\n",
       "0         0       1            0  ...                  0.000000   \n",
       "1         0       1            0  ...                  0.000000   \n",
       "2         0       1            0  ...                  0.266667   \n",
       "3         0       1            0  ...                  0.000000   \n",
       "4         0       1            0  ...                  0.266667   \n",
       "..      ...     ...          ...  ...                       ...   \n",
       "136       0       0            1  ...                  0.000000   \n",
       "137       0       0            1  ...                  0.000000   \n",
       "138       0       0            1  ...                  0.000000   \n",
       "139       0       0            1  ...                  0.000000   \n",
       "140       0       0            1  ...                  0.000000   \n",
       "\n",
       "     ethnicity_South East Asian  country_India  country_Iran  \\\n",
       "0                      0.300000       0.300000      0.000000   \n",
       "1                      0.300000       0.300000      0.000000   \n",
       "2                      0.300000       0.300000      0.266667   \n",
       "3                      0.400000       0.400000      0.000000   \n",
       "4                      0.266667       0.266667      0.266667   \n",
       "..                          ...            ...           ...   \n",
       "136                    0.566667       0.566667      0.000000   \n",
       "137                    0.600000       0.600000      0.000000   \n",
       "138                    0.366667       0.366667      0.000000   \n",
       "139                    0.466667       0.466667      0.000000   \n",
       "140                    0.200000       0.200000      0.000000   \n",
       "\n",
       "     location_San Francisco  city_Ahvaz  city_Delhi  city_Mumbai  \\\n",
       "0                  0.300000    0.000000    0.033333     0.266667   \n",
       "1                  0.300000    0.000000    0.033333     0.266667   \n",
       "2                  0.566667    0.266667    0.000000     0.300000   \n",
       "3                  0.400000    0.000000    0.100000     0.300000   \n",
       "4                  0.533333    0.266667    0.166667     0.100000   \n",
       "..                      ...         ...         ...          ...   \n",
       "136                0.566667    0.000000    0.233333     0.333333   \n",
       "137                0.600000    0.000000    0.266667     0.333333   \n",
       "138                0.366667    0.000000    0.266667     0.100000   \n",
       "139                0.466667    0.000000    0.233333     0.233333   \n",
       "140                0.200000    0.000000    0.000000     0.200000   \n",
       "\n",
       "     buddy_USF1000  buddy_USF1001  \n",
       "0         0.266667       0.033333  \n",
       "1         0.266667       0.033333  \n",
       "2         0.300000       0.266667  \n",
       "3         0.300000       0.100000  \n",
       "4         0.100000       0.433333  \n",
       "..             ...            ...  \n",
       "136       0.333333       0.233333  \n",
       "137       0.333333       0.266667  \n",
       "138       0.100000       0.266667  \n",
       "139       0.233333       0.233333  \n",
       "140       0.200000       0.000000  \n",
       "\n",
       "[165 rows x 40 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pandas.read_csv('datasets/users.csv')\n",
    "\n",
    "users_encoded = users.copy()\n",
    "users_encoded = users_encoded.drop(columns=['name', 'user_id'], axis=1)\n",
    "users_encoded['age'] = users['age'].astype(str)\n",
    "users_encoded = pandas.get_dummies(users_encoded)\n",
    "\n",
    "users_demographics = df.copy()\n",
    "for index, row in df.iterrows():\n",
    "    for col in users_encoded.columns:\n",
    "        sum = 0\n",
    "        # number_of_users is the number of rows in users_encoded\n",
    "        number_of_users = len(users_encoded.index)\n",
    "        for user_id in range(number_of_users):\n",
    "            weight = users_encoded.at[user_id, col]\n",
    "            rating = df.at[index, f'USF100{user_id}_rating']\n",
    "            sum += weight * rating\n",
    "        users_demographics.at[index, col] = sum / (number_of_users * 10)\n",
    "users_demographics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_json(\"datasets/joke_info.json\")\n",
    "USF_users = pandas.read_csv('datasets/users.csv')\n",
    "\n",
    "avg_user_rating = []\n",
    "for index, row in USF_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    avg_user_rating.append(df[f'{user_id}_rating'].mean())\n",
    "\n",
    "USF_users['avg_user_rating'] = avg_user_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "USF1002_ratings = pandas.read_csv('datasets/ratings_USF1002.csv')\n",
    "USF1002_ratings = USF1002_ratings[USF1002_ratings['joke_id'].str.startswith('JE')]\n",
    "\n",
    "USF1000_ratings = pandas.read_csv('datasets/ratings_USF1000.csv')\n",
    "USF1000_ratings = USF1000_ratings[USF1000_ratings['joke_id'].str.startswith('JE')]\n",
    "\n",
    "USF1001_ratings = pandas.read_csv('datasets/ratings_USF1001.csv')\n",
    "USF1001_ratings = USF1001_ratings[USF1001_ratings['joke_id'].str.startswith('JE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pandas.DataFrame(columns=['joke_id', 'USF1002_rating', 'USF1000_rating', 'USF1001_rating'])\n",
    "pred['joke_id'] = USF1002_ratings['joke_id']\n",
    "pred['USF1002_rating'] = USF1002_ratings['rating']\n",
    "pred['USF1000_rating'] = USF1000_ratings['rating']\n",
    "pred['USF1001_rating'] = USF1001_ratings['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_id</th>\n",
       "      <th>USF1002_rating</th>\n",
       "      <th>USF1000_rating</th>\n",
       "      <th>USF1001_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>JE40</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>JE41</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>JE42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>JE43</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>JE44</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>JE45</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>JE46</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>JE47</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>JE48</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>JE49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>JE50</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   joke_id  USF1002_rating  USF1000_rating  USF1001_rating\n",
       "39    JE40               1             1.0             9.0\n",
       "40    JE41               0             1.0             5.0\n",
       "41    JE42               0             0.0            10.0\n",
       "42    JE43               0             3.0             0.0\n",
       "43    JE44               0             5.0            10.0\n",
       "44    JE45               0             4.0            10.0\n",
       "45    JE46               0             0.0            10.0\n",
       "46    JE47               0             2.0            10.0\n",
       "47    JE48               3             3.0            10.0\n",
       "48    JE49               0             0.0             8.0\n",
       "49    JE50               0             2.0            10.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted = pred.copy()\n",
    "extracted = extracted.dropna()\n",
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_id</th>\n",
       "      <th>USF1000_rating</th>\n",
       "      <th>USF1001_rating</th>\n",
       "      <th>USF1002_rating</th>\n",
       "      <th>datasource_joke_id</th>\n",
       "      <th>jokeText</th>\n",
       "      <th>jester</th>\n",
       "      <th>reddit</th>\n",
       "      <th>stupidstuff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE5tz52q</td>\n",
       "      <td>5.10303</td>\n",
       "      <td>-1.206061</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>I hate how you cant even say black paint anymo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE5tz4dd</td>\n",
       "      <td>5.10303</td>\n",
       "      <td>-1.206061</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>5tz4dd</td>\n",
       "      <td>What's the difference between a Jew in Nazi Ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE5tz319</td>\n",
       "      <td>6.10303</td>\n",
       "      <td>-2.206061</td>\n",
       "      <td>-6.90303</td>\n",
       "      <td>5tz319</td>\n",
       "      <td>I recently went to America....\\n...and being t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RE5tz2wj</td>\n",
       "      <td>3.10303</td>\n",
       "      <td>-2.206061</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>5tz2wj</td>\n",
       "      <td>Brian raises his hand and says, “He’s in Heave...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RE5tz1pc</td>\n",
       "      <td>1.10303</td>\n",
       "      <td>3.793939</td>\n",
       "      <td>-6.90303</td>\n",
       "      <td>5tz1pc</td>\n",
       "      <td>You hear about the University book store worke...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>SS2771</td>\n",
       "      <td>-0.89697</td>\n",
       "      <td>-3.206061</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>2771</td>\n",
       "      <td>Q: How many Field Service Engineers does it ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>SS1365</td>\n",
       "      <td>-1.89697</td>\n",
       "      <td>-3.206061</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>1365</td>\n",
       "      <td>Yo mamma so dumb she sold her car for gasoline...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>SS570</td>\n",
       "      <td>-1.89697</td>\n",
       "      <td>3.793939</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>570</td>\n",
       "      <td>A woman gets home, schreeches her car into the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>SS431</td>\n",
       "      <td>-0.89697</td>\n",
       "      <td>-0.206061</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>431</td>\n",
       "      <td>The psychology instructor had just finished a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>SS1460</td>\n",
       "      <td>6.10303</td>\n",
       "      <td>0.793939</td>\n",
       "      <td>1.09697</td>\n",
       "      <td>1460</td>\n",
       "      <td>Your mama is so fat, she fell in love and brok...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      joke_id  USF1000_rating  USF1001_rating  USF1002_rating  \\\n",
       "0    RE5tz52q         5.10303       -1.206061         1.09697   \n",
       "1    RE5tz4dd         5.10303       -1.206061         1.09697   \n",
       "2    RE5tz319         6.10303       -2.206061        -6.90303   \n",
       "3    RE5tz2wj         3.10303       -2.206061         1.09697   \n",
       "4    RE5tz1pc         1.10303        3.793939        -6.90303   \n",
       "..        ...             ...             ...             ...   \n",
       "160    SS2771        -0.89697       -3.206061         1.09697   \n",
       "161    SS1365        -1.89697       -3.206061         1.09697   \n",
       "162     SS570        -1.89697        3.793939         1.09697   \n",
       "163     SS431        -0.89697       -0.206061         1.09697   \n",
       "164    SS1460         6.10303        0.793939         1.09697   \n",
       "\n",
       "    datasource_joke_id                                           jokeText  \\\n",
       "0               5tz52q  I hate how you cant even say black paint anymo...   \n",
       "1               5tz4dd  What's the difference between a Jew in Nazi Ge...   \n",
       "2               5tz319  I recently went to America....\\n...and being t...   \n",
       "3               5tz2wj  Brian raises his hand and says, “He’s in Heave...   \n",
       "4               5tz1pc  You hear about the University book store worke...   \n",
       "..                 ...                                                ...   \n",
       "160               2771  Q: How many Field Service Engineers does it ta...   \n",
       "161               1365  Yo mamma so dumb she sold her car for gasoline...   \n",
       "162                570  A woman gets home, schreeches her car into the...   \n",
       "163                431  The psychology instructor had just finished a ...   \n",
       "164               1460  Your mama is so fat, she fell in love and brok...   \n",
       "\n",
       "     jester  reddit  stupidstuff  \n",
       "0         0       1            0  \n",
       "1         0       1            0  \n",
       "2         0       1            0  \n",
       "3         0       1            0  \n",
       "4         0       1            0  \n",
       "..      ...     ...          ...  \n",
       "160       0       0            1  \n",
       "161       0       0            1  \n",
       "162       0       0            1  \n",
       "163       0       0            1  \n",
       "164       0       0            1  \n",
       "\n",
       "[165 rows x 9 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a new  dataframe df2 that recalculates the ratings of all jokes by calculating the difference between the average rating of the user and the rating of the joke\n",
    "# so for a user with id x in USF_users, the value of `x_rating` should be the difference between the average rating of the user from the USF_users dataframe and the rating of the joke from the df dataframe\n",
    "# if a user has not rated any joke, the value of `x_rating` should be 0\n",
    "\n",
    "df2 = df.copy()\n",
    "for index, row in USF_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    df2[f'{user_id}_rating'] =  row['avg_user_rating'] - df2[f'{user_id}_rating']\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>USF1000</th>\n",
       "      <th>USF1001</th>\n",
       "      <th>USF1002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USF1000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257828</td>\n",
       "      <td>-0.109062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USF1001</th>\n",
       "      <td>0.257828</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USF1002</th>\n",
       "      <td>-0.109062</td>\n",
       "      <td>0.05376</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id   USF1000   USF1001   USF1002\n",
       "user_id                              \n",
       "USF1000       1.0  0.257828 -0.109062\n",
       "USF1001  0.257828       1.0   0.05376\n",
       "USF1002 -0.109062   0.05376       1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = pandas.DataFrame(index=USF_users['user_id'], columns=USF_users['user_id'])\n",
    "\n",
    "for index, row in USF_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    for index2, row2 in USF_users.iterrows():\n",
    "        user_id2 = row2['user_id']\n",
    "        similarity_matrix[user_id][user_id2] = (df2[f'{user_id}_rating'] * df2[f'{user_id2}_rating']).sum() / (numpy.sqrt((df2[f'{user_id}_rating']**2).sum()) * numpy.sqrt((df2[f'{user_id2}_rating']**2).sum()))\n",
    "\n",
    "similarity_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate buddy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>country</th>\n",
       "      <th>location</th>\n",
       "      <th>avg_user_rating</th>\n",
       "      <th>city</th>\n",
       "      <th>buddy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USF1000</td>\n",
       "      <td>Tanya</td>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>South East Asian</td>\n",
       "      <td>India</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>6.103030</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>USF1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USF1001</td>\n",
       "      <td>rishab</td>\n",
       "      <td>26</td>\n",
       "      <td>Male</td>\n",
       "      <td>South East Asian</td>\n",
       "      <td>India</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>6.793939</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>USF1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USF1002</td>\n",
       "      <td>Parisa</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>Middle Eastern</td>\n",
       "      <td>Iran</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>1.096970</td>\n",
       "      <td>Ahvaz</td>\n",
       "      <td>USF1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    name  age  gender         ethnicity country       location  \\\n",
       "0  USF1000   Tanya   24  Female  South East Asian   India  San Francisco   \n",
       "1  USF1001  rishab   26    Male  South East Asian   India  San Francisco   \n",
       "2  USF1002  Parisa   26  Female    Middle Eastern    Iran  San Francisco   \n",
       "\n",
       "   avg_user_rating    city    buddy  \n",
       "0         6.103030   Delhi  USF1001  \n",
       "1         6.793939  Mumbai  USF1000  \n",
       "2         1.096970   Ahvaz  USF1001  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USF_users['buddy'] = ''\n",
    "for index, row in USF_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    max_similarity = 0\n",
    "    buddy = ''\n",
    "    for index2, row2 in USF_users.iterrows():\n",
    "        user_id2 = row2['user_id']\n",
    "        if user_id2 != user_id and similarity_matrix[user_id][user_id2] > max_similarity:\n",
    "            max_similarity = similarity_matrix[user_id][user_id2]\n",
    "            buddy = user_id2\n",
    "    USF_users.loc[index, 'buddy'] = buddy\n",
    "\n",
    "USF_users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 from this point onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = users_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is our target user. Currently we're targeting jokes for Tanya -> USF1000\n",
    "\n",
    "curr_user_traits = {\n",
    "    \"age\": 24,\n",
    "    \"gender\": \"Female\",\n",
    "    \"ethnicity\": \"South East Asian\",\n",
    "    \"country\": \"India\",\n",
    "    \"location\" : \"San Francisco\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# These traits tell us which columns to use from the user_demographics dataframe\n",
    "\n",
    "col_names_to_use = [ str(key) + '_' + str(value) for key, value in curr_user_traits.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in col_names_to_use:\n",
    "#     result_df = df[df[col] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = col_names_to_use\n",
    "\n",
    "columns_to_keep.extend([\n",
    "    # 'USF1000_rating', \n",
    "    # 'USF1001_rating', \n",
    "    # 'USF1002_rating',\n",
    "    # \"jester\",\n",
    "    # \"reddit\",\n",
    "    # \"stupidstuff\",\n",
    "    # 'joke_id',\n",
    "    # 'jokeText', \n",
    "    'jester', \n",
    "    'reddit', \n",
    "    'stupidstuff',\n",
    "    'toxic', \n",
    "    'severe_toxic', \n",
    "    'obscene', \n",
    "    'threat', \n",
    "    'insult', \n",
    "    'identity_hate',\n",
    "    'anger', \n",
    "    'disgust', \n",
    "    'fear', \n",
    "    'joy', \n",
    "    'neutral', \n",
    "    'sadness', \n",
    "    'surprise',\n",
    "    'misogynist', \n",
    "    'sexist', \n",
    "])\n",
    "\n",
    "# label_df = temp_df['USF1002_rating'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# result_df = result_df[result_df['USF1002_rating']]\n",
    "\n",
    "# label_df = result_df['USF1002_rating']\n",
    "\n",
    "# result_df = result_df[columns_to_keep]\n",
    "\n",
    "## Since the target user is USF1000 now\n",
    "\n",
    "label_df_demographics = df['USF1000_rating']\n",
    "\n",
    "features_df_demographics = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df_demographics, label_df_demographics , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{'criterion': 'squared_error', 'max_depth': 5, 'n_estimators': 150}\n",
      "Accuracy-> 0.997931715210356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# params = {'max_depth': [3, 4, 5], # Go smaller\n",
    "#           'criterion': ['entropy', 'gini'],\n",
    "#           'n_estimators': [115, 130, 150], # Go smaller, 3/5/10,x,\n",
    "#           'max_features': [None]\n",
    "#         }\n",
    "\n",
    "# params = {'max_depth': [None, 5, 10, 20, 30], \n",
    "#           'criterion': ['absolute_error', 'squared_error'],\n",
    "#           'n_estimators': [115, 130, 150, 200], \n",
    "#           'max_features': [None, 'log2']\n",
    "#         }\n",
    "\n",
    "params = {'max_depth': [5, 10, 9],\n",
    "          'criterion': ['squared_error'],\n",
    "          'n_estimators': [50, 150, 200]\n",
    "        }\n",
    "\n",
    "\n",
    "folds = 5\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "clf = GridSearchCV(rf, params, cv=folds, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "rf_demographics = RandomForestRegressor(**clf.best_params_, random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "rf_demographics.fit(X_train, y_train)\n",
    "y_pred = rf_demographics.predict(X_test)\n",
    "\n",
    "accuracy = rf_demographics.score(X_test, y_test)\n",
    "print(\"Accuracy->\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "Step 5\n",
    "-------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_df = users_demographics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a dict of target user traits\n",
    "\n",
    "target_user = \"USF1000\"\n",
    "\n",
    "target_user_traits = users[users['user_id'] == target_user].to_dict('records')[0]\n",
    "\n",
    "## Get info of taget user's buddy\n",
    "\n",
    "target_user_buddy_traits = users[users['user_id'] == target_user_traits['buddy']].to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns to keep for our model \n",
    "\n",
    "relevant_columns = set()\n",
    "\n",
    "traits_to_ignore = ['user_id', 'buddy', 'name', 'avg_user_rating', 'city']\n",
    "\n",
    "for key, value in target_user_traits.items():\n",
    "    if key not in traits_to_ignore:\n",
    "        relevant_columns.add(str(key) + \"_\" + str(value))\n",
    "\n",
    "\n",
    "for key, value in target_user_buddy_traits.items():\n",
    "    if key not in traits_to_ignore:\n",
    "        relevant_columns.add(str(key) + \"_\" + str(value))\n",
    "\n",
    "relevant_columns = list(relevant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.30\n",
    "\n",
    "for col in relevant_columns:\n",
    "    jokes_df = jokes_df[jokes_df[col] > similarity_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USF1000_rating</th>\n",
       "      <th>jester</th>\n",
       "      <th>reddit</th>\n",
       "      <th>stupidstuff</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>...</th>\n",
       "      <th>surprise</th>\n",
       "      <th>misogynist</th>\n",
       "      <th>sexist</th>\n",
       "      <th>ethnicity_South East Asian</th>\n",
       "      <th>location_San Francisco</th>\n",
       "      <th>age_24</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>country_India</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>age_26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631412</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.265192</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.076584</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059827</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.078934</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636481</td>\n",
       "      <td>0.047341</td>\n",
       "      <td>0.107221</td>\n",
       "      <td>0.051621</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.067843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037257</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.771947</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.067323</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.147464</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.481255</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486036</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.343611</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.141612</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.104133</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818475</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.170631</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.349303</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     USF1000_rating  jester  reddit  stupidstuff     toxic  severe_toxic  \\\n",
       "124            10.0       0       0            1  0.631412      0.005822   \n",
       "127            10.0       0       0            1  0.636481      0.047341   \n",
       "129            10.0       0       0            1  0.771947      0.001812   \n",
       "130            10.0       0       0            1  0.486036      0.007341   \n",
       "131            10.0       0       0            1  0.818475      0.000337   \n",
       "\n",
       "      obscene    threat    insult  identity_hate  ...  surprise  misogynist  \\\n",
       "124  0.265192  0.005896  0.076584       0.015095  ...  0.059827    0.003128   \n",
       "127  0.107221  0.051621  0.089494       0.067843  ...  0.037257    0.000158   \n",
       "129  0.067323  0.002093  0.147464       0.009362  ...  0.015873    0.000238   \n",
       "130  0.343611  0.003386  0.141612       0.018014  ...  0.012207    0.000258   \n",
       "131  0.008088  0.000244  0.170631       0.002225  ...  0.003500    0.999519   \n",
       "\n",
       "       sexist  ethnicity_South East Asian  location_San Francisco    age_24  \\\n",
       "124  0.078934                    0.666667                0.666667  0.333333   \n",
       "127  0.010693                    0.666667                0.666667  0.333333   \n",
       "129  0.481255                    0.666667                0.666667  0.333333   \n",
       "130  0.104133                    0.666667                0.666667  0.333333   \n",
       "131  0.349303                    0.666667                0.666667  0.333333   \n",
       "\n",
       "     gender_Male  country_India  gender_Female    age_26  \n",
       "124     0.333333       0.666667       0.333333  0.333333  \n",
       "127     0.333333       0.666667       0.333333  0.333333  \n",
       "129     0.333333       0.666667       0.333333  0.333333  \n",
       "130     0.333333       0.666667       0.333333  0.333333  \n",
       "131     0.333333       0.666667       0.333333  0.333333  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_relevant_columns = [\n",
    "    target_user_traits['user_id'] + '_rating',\n",
    "    # target_user_buddy_traits['user_id'] + '_rating',\n",
    "    # 'joke_id',\n",
    "    # 'datasource_joke_id',\n",
    "    # 'jokeText',\n",
    "    'jester', \n",
    "    'reddit', \n",
    "    'stupidstuff',\n",
    "    'toxic', \n",
    "    'severe_toxic', \n",
    "    'obscene', \n",
    "    'threat', \n",
    "    'insult', \n",
    "    'identity_hate',\n",
    "    'anger', \n",
    "    'disgust', \n",
    "    'fear', \n",
    "    'joy', \n",
    "    'neutral', \n",
    "    'sadness', \n",
    "    'surprise',\n",
    "    'misogynist', \n",
    "    'sexist'\n",
    "]\n",
    "\n",
    "original_relevant_columns.extend(relevant_columns)\n",
    "\n",
    "jokes_df = jokes_df[original_relevant_columns]\n",
    "\n",
    "jokes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'USF1000_rating'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'USF1000_rating'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m label_df_buddy \u001b[39m=\u001b[39m jokes_df[target_user_traits[\u001b[39m'\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_rating\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      4\u001b[0m jokes_df \u001b[39m=\u001b[39m jokes_df\u001b[39m.\u001b[39mdrop([target_user_traits[\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_rating\u001b[39m\u001b[39m'\u001b[39m, target_user_traits[\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_rating\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m X_train_buddy, X_test_buddy, y_train_buddy, y_test_buddy \u001b[39m=\u001b[39m train_test_split(jokes_df, label_df_buddy , test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'USF1000_rating'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_df_buddy = jokes_df[target_user_traits['user_id'] + '_rating']\n",
    "jokes_df = jokes_df.drop([target_user_traits['user_id'] + '_rating', target_user_traits['user_id'] + '_rating'], axis=1)\n",
    "\n",
    "X_train_buddy, X_test_buddy, y_train_buddy, y_test_buddy = train_test_split(jokes_df, label_df_buddy , test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m rf_buddy \u001b[39m=\u001b[39m RandomForestRegressor()\n\u001b[1;32m     26\u001b[0m clf \u001b[39m=\u001b[39m GridSearchCV(rf_buddy, params, cv\u001b[39m=\u001b[39mfolds, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_buddy, y_train_buddy, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mn_splits\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m4\u001b[39;49m})\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(clf\u001b[39m.\u001b[39mbest_params_)\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestRegressor, RandomForestClassifier\n",
      "File \u001b[0;32m/Library/Python/3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Python/3.9/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/Library/Python/3.9/site-packages/sklearn/model_selection/_search.py:833\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[0;32m--> 833\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Python/3.9/site-packages/sklearn/model_selection/_split.py:345\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    343\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m         (\n\u001b[1;32m    347\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have number of splits n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m greater\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m than the number of samples: n_samples=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         )\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    353\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "\n",
    "# params = {'max_depth': [3, 4, 5], # Go smaller\n",
    "#           'criterion': ['entropy', 'gini'],\n",
    "#           'n_estimators': [115, 130, 150], # Go smaller, 3/5/10,x,\n",
    "#           'max_features': [None]\n",
    "#         }\n",
    "\n",
    "# params = {'max_depth': [None, 5, 10, 20, 30], \n",
    "#           'criterion': ['absolute_error', 'squared_error'],\n",
    "#           'n_estimators': [115, 130, 150, 200], \n",
    "#           'max_features': [None, 'log2']\n",
    "#         }\n",
    "\n",
    "params = {'max_depth': [5, 10, 9],\n",
    "          'criterion': ['squared_error'],\n",
    "          'n_estimators': [50, 150, 200]\n",
    "        }\n",
    "\n",
    "\n",
    "folds = 5\n",
    "rf_buddy = RandomForestRegressor()\n",
    "\n",
    "clf = GridSearchCV(rf_buddy, params, cv=folds, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train_buddy, y_train_buddy, **{'n_splits': 4})\n",
    "print(clf.best_params_)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Train the model with the best parameters obtained\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rf = RandomForestRegressor(**clf.best_params_, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train_buddy, y_train_buddy)\n",
    "y_pred = rf.predict(X_test_buddy)\n",
    "\n",
    "accuracy = rf.score(X_test_buddy, y_test_buddy)\n",
    "print(\"Accuracy->\", accuracy)\n",
    "\n",
    "rmse = mean_squared_error(y_test_buddy, y_pred, squared=False)\n",
    "print(\"RMSE->\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jester</th>\n",
       "      <th>reddit</th>\n",
       "      <th>stupidstuff</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>anger</th>\n",
       "      <th>...</th>\n",
       "      <th>surprise</th>\n",
       "      <th>misogynist</th>\n",
       "      <th>sexist</th>\n",
       "      <th>ethnicity_South East Asian</th>\n",
       "      <th>location_San Francisco</th>\n",
       "      <th>age_24</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>country_India</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>age_26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818475</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.170631</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.349303</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.771947</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.067323</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.147464</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.937975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.481255</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631412</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.265192</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.076584</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>0.543601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059827</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.078934</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486036</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.343611</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.141612</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>0.026760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.104133</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     jester  reddit  stupidstuff     toxic  severe_toxic   obscene    threat  \\\n",
       "131       0       0            1  0.818475      0.000337  0.008088  0.000244   \n",
       "129       0       0            1  0.771947      0.001812  0.067323  0.002093   \n",
       "124       0       0            1  0.631412      0.005822  0.265192  0.005896   \n",
       "130       0       0            1  0.486036      0.007341  0.343611  0.003386   \n",
       "\n",
       "       insult  identity_hate     anger  ...  surprise  misogynist    sexist  \\\n",
       "131  0.170631       0.002225  0.020293  ...  0.003500    0.999519  0.349303   \n",
       "129  0.147464       0.009362  0.937975  ...  0.015873    0.000238  0.481255   \n",
       "124  0.076584       0.015095  0.543601  ...  0.059827    0.003128  0.078934   \n",
       "130  0.141612       0.018014  0.026760  ...  0.012207    0.000258  0.104133   \n",
       "\n",
       "     ethnicity_South East Asian  location_San Francisco    age_24  \\\n",
       "131                    0.666667                0.666667  0.333333   \n",
       "129                    0.666667                0.666667  0.333333   \n",
       "124                    0.666667                0.666667  0.333333   \n",
       "130                    0.666667                0.666667  0.333333   \n",
       "\n",
       "     gender_Male  country_India  gender_Female    age_26  \n",
       "131     0.333333       0.666667       0.333333  0.333333  \n",
       "129     0.333333       0.666667       0.333333  0.333333  \n",
       "124     0.333333       0.666667       0.333333  0.333333  \n",
       "130     0.333333       0.666667       0.333333  0.333333  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
